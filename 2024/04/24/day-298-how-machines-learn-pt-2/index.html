<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Day 298: How Machines Learn Pt. 2 | DangerousDayJob </title> <meta name="author" content="Benjamin Hawley"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://hawleybenjamin1.github.io/2024/04/24/day-298-how-machines-learn-pt-2/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> DangerousDayJob </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/publications/">publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/projects/">projects</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Day 298: How Machines Learn Pt. 2</h1> <p class="post-meta"> Created in April 24, 2024 by {"login"=&gt;"bhawleywrites", "email"=&gt;"hawleybenjamin1@gmail.com", "display_name"=&gt;"bhawleywrites", "first_name"=&gt;"", "last_name"=&gt;""} • {"_jetpack_memberships_contains_paywalled_content"=&gt;"", "_jetpack_memberships_contains_paid_content"=&gt;"", "_last_editor_used_jetpack"=&gt;"block-editor", "wordads_ufa"=&gt;"s:wpcom-ufa-v4:1713988841", "firehose_sent"=&gt;"1713988557", "timeline_notification"=&gt;"1713988558", "_publicize_job_id"=&gt;"93941415519", "_oembed_fb4193ff519a2ac4d23d3de0cfebb124"=&gt;"</p> <div class='\"embed-youtube\"'><iframe title='\"Neural' networks from scratch p.5 hidden layer activation functions width='\"500\"' height='\"281\"' src='\"https://www.youtube.com/embed/gmjzbpSVY1A?start=540&amp;feature=oembed\"' frameborder='\"0\"' allow='\"accelerometer;' autoplay clipboard-write encrypted-media gyroscope picture-in-picture web-share referrerpolicy='\"strict-origin-when-cross-origin\"' allowfullscreen></iframe></div>", "_oembed_time_fb4193ff519a2ac4d23d3de0cfebb124"=&gt;"1724994357", "_oembed_1a121db0e189094971ec86e29e87fefa"=&gt;"<div class='\"embed-youtube\"'><iframe title='\"Neural' networks from scratch p.5 hidden layer activation functions width='\"750\"' height='\"422\"' src='\"https://www.youtube.com/embed/gmjzbpSVY1A?start=540&amp;feature=oembed\"' frameborder='\"0\"' allow='\"accelerometer;' autoplay clipboard-write encrypted-media gyroscope picture-in-picture web-share referrerpolicy='\"strict-origin-when-cross-origin\"' allowfullscreen></iframe></div>", "_oembed_time_1a121db0e189094971ec86e29e87fefa"=&gt;"1731473561", "_elasticsearch_data_sharing_indexed_on"=&gt;"2024-07-17 14:19:34"} <p class="post-tags"> <i class="fa-solid fa-calendar fa-sm"></i> 2024   ·   <i class="fa-solid fa-hashtag fa-sm"></i> author   <i class="fa-solid fa-hashtag fa-sm"></i> authors   <i class="fa-solid fa-hashtag fa-sm"></i> blog   <i class="fa-solid fa-hashtag fa-sm"></i> books   <i class="fa-solid fa-hashtag fa-sm"></i> dangerousdayjob   <i class="fa-solid fa-hashtag fa-sm"></i> deep-learning   <i class="fa-solid fa-hashtag fa-sm"></i> machine-learning   <i class="fa-solid fa-hashtag fa-sm"></i> neural-networks   <i class="fa-solid fa-hashtag fa-sm"></i> writing   ·   <i class="fa-solid fa-tag fa-sm"></i> Daily </p> </header> <article class="post-content"> <div id="markdown-content"> <p></p> <p>Yesterday I started on a rather convoluted topic, but hopefully if you wrapped your head around what a layer does to the inputs it receives, then today's post should be easier to understand. Here was the output of the layer from yesterday's example.</p> <p></p> <p></p> <figure class="wp-block-table"> <table class="has-fixed-layout"> <tbody> <tr></tr> </tbody> </table> <th>﻿</th> <th class="has-text-align-center" data-align="center">N1</th> <th class="has-text-align-center" data-align="center">N2</th> <th class="has-text-align-center" data-align="center">N3</th> <th class="has-text-align-center" data-align="center">N4</th> <td class="has-text-align-center" data-align="center"></td> <strong>N5</strong><br> <tr></tr> <th>Output</th> <td class="has-text-align-center" data-align="center">0.62</td> <td class="has-text-align-center" data-align="center">0.455</td> <td class="has-text-align-center" data-align="center">0.975</td> <td class="has-text-align-center" data-align="center">0.18</td> <td class="has-text-align-center" data-align="center">0.125</td> </figure> <p></p> <p></p> <p>What we'd like to do is pass these values to a new layer. Recall that each layer has neurons with weights and biases, like knobs on a large machine. Passing these values to a new layer would allow even finer control over the effect the network has on the inputs. Deciding how many layers is a whole other topic, but for now we can just assume that two layers will be better than one. In fact, two layers between the first input and the final output is pretty much required for reasons I'll try to explain right now.</p> <p></p> <p></p> <p>We can't simply pass these inputs along to a new layer, or to the final output layer. As stated yesterday, the function these neurons apply to their inputs is this:</p> <p></p> <p></p> <p>output = inputs x weights + bias</p> <p></p> <p></p> <p>This is identical to the equation of a line.</p> <p></p> <p></p> <p>y = mx + b</p> <p></p> <p></p> <p>Where the weights determine the slope of the output line (m), and the bias determines the intercept of the line (b). Currently, all our neural network can do is take inputs and turn them into lines. No matter how many layers we stack up on top of each other, the output will always look like a line because all we are doing is multiplying previous inputs by a new slope. Most data in the real world does not have a perfectly linear relationship. </p> <p></p> <p></p> <p>The whole thing is useless! That is, if we leave it as is. To solve this, the linearity of the system must be broken. There needs to be a way to introduce curves or kinks into these lines. What if we could have a neuron or a couple of neurons only define one section of a line? Then we could make a curve that's just a bunch of tiny line segments. We might then say that some neurons are <strong>active </strong>for certain inputs. I.E any given neuron can only contribute to the line when we want them to. This is exactly what an <strong>activation function</strong> achieves.</p> <p></p> <p></p> <p>An activation function sits between layers of neurons and decides which neurons will get to play their part. It's part of the picture that's missing from diagrams like these:</p> <p></p> <p></p> <figure class="wp-block-image size-large"><a href="https://www.geeksforgeeks.org/artificial-neural-networks-and-its-applications/" rel="external nofollow noopener" target="_blank"></a><img src="/assets/2024/04/image-12.png?w=800" alt="" class="wp-image-2833"></figure> <p></p> <p></p> <p>In fact I couldn't find a single diagram that indicates the existence of an activation function between neurons. They're such an integral part of these networks it's kinda hard to believe nobody includes them in the diagram. </p> <p></p> <p></p> <p>Anyway here's a visual for how they work.</p> <p></p> <p></p> <p>First we just start with a line. If we take the input and our entire neural network was active at the same time, we'd get something like this:</p> <p></p> <p></p> <figure class="wp-block-image size-large"><img src="/assets/2024/04/image-14.png?w=1024" alt="" class="wp-image-2839"></figure> <p></p> <p></p> <p>Those sliders, w1 and b1 determine the slope and position of the line. Those could be just one weight, or they could be every weight and bias added together. If every neuron is always active, all the neurons' weights and biases will end up multiplied or added to the final output. That's why we'll always get a line (or at least, a representation of a linear relationship) without any activation function.</p> <p></p> <p></p> <p>Changing weight:</p> <p></p> <p></p> <figure class="wp-block-image size-large is-resized"><img src="/assets/2024/04/lineslopegif-2.gif?w=462" alt="" class="wp-image-2859" style="width:649px;height:auto"></figure> <p></p> <p></p> <p>Changing bias:</p> <p></p> <p></p> <figure class="wp-block-image size-large is-resized"><img src="/assets/2024/04/lineinterceptgif-4.gif?w=600" alt="" class="wp-image-2858" style="width:649px;height:auto"></figure> <p></p> <p></p> <p>Today we'll be looking at the rectified linear or ReLI activation function. All it does is cut a line off where it's output is not greater than 0. This is enough to break linearity and allow us to make some more interesting shapes.</p> <p></p> <p></p> <p>This next line is what happens when the activation function is applied it's active where the original input is between 0 and infinity.</p> <p></p> <p></p> <figure class="wp-block-image size-large"><img src="/assets/2024/04/image-16.png?w=1024" alt="" class="wp-image-2850"></figure> <p></p> <p></p> <p>Notice these aren't completely new lines, I'm taking the output of the previous function and doing new operations to it, just like the neural network will.</p> <p></p> <p></p> <p>Now what if I want a line that's active only over a constrained area like we talked about before? I need to do a few operations. First I'll pass the output of the activation function to a new neuron, and use the weight and bias to set some limits.</p> <p></p> <p></p> <figure class="wp-block-image size-large"><img src="/assets/2024/04/image-17.png?w=1024" alt="" class="wp-image-2851"></figure> <p></p> <p></p> <p>Then I have to pass this to another activation function.</p> <p></p> <p></p> <figure class="wp-block-image size-large"><img src="/assets/2024/04/image-18.png?w=1024" alt="" class="wp-image-2853"></figure> <p></p> <p></p> <p>And there you go, the output is no longer just a single line. I can even reverse this again by applying one last weight to it as it's accepted by the output neuron to get this.</p> <p></p> <p></p> <figure class="wp-block-image size-large"><img src="/assets/2024/04/image-19.png?w=1024" alt="" class="wp-image-2855"></figure> <p></p> <p></p> <p>This is about as simple as I can possibly get. This neural 'network' would just be four neurons. Input neuron that receives X values, two hidden neurons, and a final output neuron that gives us the Y values as displayed in the calculator. Stack several of these neuron pairs together and you can make a curve, as demonstrated in this video with much more depth:</p> <p></p> <p></p> <figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"> <div class="wp-block-embed__wrapper"> https://youtu.be/gmjzbpSVY1A?si=FpkewgdGRaFy6rex&amp;t=540 </div> </figure> <p></p> <p></p> <p>Something to keep in mind is that these outputs are lines because I've used two neurons, or as he does in the video, neuron pairs. For a dense layer as we explored yesterday, the output could be reduced to lines at the very end if that were your prerogative, but the actual output of each individual layer is impossible to visualize. Those matrix multiplications we were doing yesterday can get into the hundreds of dimensions. This was a simplified example for the sake of figuring out what the heck is going on in there. </p> <p></p> <p></p> <p>That's about it for the rectified linear activation function. Tomorrow I'll finish this neural network series up with one more post, and then I'll move on to my usual book review and literary content. I wanted to give more technical writing a try and found out it's pretty hard. I'll have some good material to look back on and learn from, and maybe in the future I'll give some other technical concept a try. </p> <p></p> <p></p> <p>Thank you for reading,</p> <p></p> <p></p> <p>Benjamin Hawley</p> <p></p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/displaying-external-posts-on-your-al-folio-blog/">Displaying External Posts on Your al-folio Blog</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2025/01/07/day-472-well/">Day 472: Well ...</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2025/01/06/day-471-back-to-work/">Day 471: Back to Work</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2025/01/03/happy-friday/">Happy Friday</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Benjamin Hawley. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?70d799092f862ad98c7876aa47712e20"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>